{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- implement the gradient-based learning using sampling\n",
    "- implement the EM method\n",
    "- compare the two using synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Our goal in this exercise is to learn the parameters of a model from incomplete data. The model is a mixture of Guassians. According to this model, each data point is sampled from one of $C$ Gaussian distributions, according to the class of that data point. The prior probability for class label follows a multinomial distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Z_i = j) = p_j \n",
    "\\end{equation}\n",
    "\n",
    "Given the class of a data point, its value follows a normal distribution with parameters that are specific to that class:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y_i = y | Z_i = j) = N(y; \\mu_j, \\sigma_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the latent variables\n",
    "For each data point $Y_i$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Z_i=j|Y_i=y_i;\\Theta) & \\propto P(Y_i=y_i|Z_i=j;\\Theta)P(Z_i=j;\\Theta)   \\\\\n",
    "                  & = N(y_i;\\mu_j,\\sigma_j) \\cdot p_j\n",
    "\\end{align*}\n",
    "\n",
    "Having the posterior of class label, we can sample the label from this posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy.random import multinomial\n",
    "\n",
    "# returns a vector with counts of each class\n",
    "def sample_label(parameters, y, num_samples=1):\n",
    "    multinomial_parameters, normal_parameters = parameters\n",
    "    assert len(multinomial_parameters) == len(normal_parameters)\n",
    "    num_classes = len(multinomial_parameters)\n",
    "    \n",
    "    posterior = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        mu, sigma = normal_parameters[i]\n",
    "        p = multinomial_parameters[i]\n",
    "        posterior[i] = norm.pdf(y, loc=mu, scale=sigma) * p\n",
    "    s = sum(posterior)\n",
    "    posterior = [ps/s for ps in posterior]\n",
    "    return multinomial(num_samples, posterior)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us test the correctness of gradients of normal distribution. Given $f(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$, I believe that:\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\mu} &= f . \\frac{1}{\\sigma} . \\big(\\frac{x-\\mu}{\\sigma}\\big) \\\\\n",
    "\\frac{\\partial f}{\\partial \\sigma} &= f . \\frac{1}{\\sigma} . \\big( (\\frac{x-\\mu}{\\sigma})^2 -1 \\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "from scipy.stats import norm\n",
    "from numpy.random import random_sample, normal\n",
    "\n",
    "def f_normal(x):\n",
    "    def _f(inp):\n",
    "        mu, sigma = inp\n",
    "        return norm.pdf(x, loc=mu, scale=sigma)\n",
    "    return _f\n",
    "\n",
    "def fprime_normal(x):\n",
    "    def _fprime(inp):\n",
    "        f_val = f(x)(inp)\n",
    "        mu, sigma = inp\n",
    "        xms = (x-mu)/sigma\n",
    "        \n",
    "        grad=[0.0]*2\n",
    "        grad[0] = f_val/sigma * xms\n",
    "        grad[1] = f_val/sigma * (xms**2 - 1)\n",
    "        return grad\n",
    "    return _fprime\n",
    "\n",
    "# for i in range(10000):\n",
    "#     mu = 30 * random_sample()\n",
    "#     sigma = 10 * random_sample()\n",
    "#     val = normal(mu, sigma)\n",
    "#     diff =  check_grad(f(val), fprime(val), [mu, sigma])\n",
    "#     if diff > 1e-4:\n",
    "#         print mu, sigma, val, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2928469665 0.00592172043732 28.2863311113 0.000598214503615\n",
      "8.96909573673 0.000621093559227 8.96901323197 0.0266009950405\n",
      "28.1549881946 0.0068173659069 28.1529651645 0.000199039264796\n",
      "9.80309539067 0.00749007908259 9.81579395677 0.00102107474286\n",
      "19.9941297072 0.00911564481845 19.9917446729 0.00011452526983\n",
      "25.5092916176 0.0221553537523 25.4729479686 0.000108410052508\n",
      "27.9116999558 0.00186872685539 27.9092031722 0.00953395716171\n",
      "10.0643932165 0.00784502673061 10.0624190567 0.000155770172607\n",
      "29.1572354162 0.00705287246878 29.1434574364 0.00157213412558\n",
      "2.32903647632 0.00168510148118 2.32796070813 0.0026881099904\n",
      "7.45309686299 0.01909445103 7.41427392373 0.000233930416804\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random_sample, normal\n",
    "from scipy.stats import norm\n",
    "\n",
    "def f_normal_log(x):\n",
    "    return lambda y : norm.logpdf(x, loc=y[0], scale=y[1])\n",
    "\n",
    "def fprime_normal_log(x):\n",
    "    def fprime_log_(y):\n",
    "        mu, sigma = y\n",
    "        xms = (x-mu)/sigma\n",
    "        fprime_mu = xms / sigma\n",
    "        fprime_sigma = (xms**2 - 1) / sigma\n",
    "        return (fprime_mu, fprime_sigma)\n",
    "    return fprime_log_\n",
    "\n",
    "for i in range(5000):\n",
    "    mu = 30 * random_sample()\n",
    "    sigma = 10 * random_sample()\n",
    "    val = normal(mu, sigma)\n",
    "    diff =  check_grad(f_normal_log(val), fprime_normal_log(val), [mu, sigma])\n",
    "    if diff > 1e-4:\n",
    "        print mu, sigma, val, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_multinomial(f_multinomial_pos, fprime_multinomial_pos, \n",
    "                    num_tests = 1000):\n",
    "    num_classes = np.random.randint(40)\n",
    "    for i in range(num_tests):\n",
    "        params = [normal(loc=0, scale=1) for i in range(num_classes)]\n",
    "        p2 = numpy.exp(params)\n",
    "        p2 = [i/sum(p2) for i in p2]\n",
    "        val = multinomial(1, p2)\n",
    "        val = numpy.where(val==1)[0][0]\n",
    "        diff =  check_grad(f_multinomial_pos(val), fprime_multinomial_pos(val), params)\n",
    "        if diff > 1e-4:\n",
    "            print params, val, diff\n",
    "            print f_multinomial_pos(val)(params) \n",
    "            print fprime_multinomial_pos(val)(params)\n",
    "            print approx_fprime(params, f_multinomial_pos(val), 1.49e-08)\n",
    "            print \n",
    "    print 'test finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test finished\n"
     ]
    }
   ],
   "source": [
    "def f_multinomial_pos(label):\n",
    "    return (lambda parameters : np.exp(parameters[label]) / sum(np.exp(parameters)))\n",
    "\n",
    "def fprime_multinomial_pos(label):\n",
    "    def _fprime_multinomial_pos(parameters):\n",
    "        f = [f_multinomial_pos(i)(parameters) for i in range(len(parameters))]\n",
    "        f_i = f[label]\n",
    "        fprime = [-f_i*f_j for f_j in f]\n",
    "        fprime[label] = f_i*(1-f_i)\n",
    "        return fprime\n",
    "    return _fprime_multinomial_pos\n",
    "\n",
    "test_multinomial(f_multinomial_pos, fprime_multinomial_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test finished\n"
     ]
    }
   ],
   "source": [
    "def f_multinomial_log(label):\n",
    "    return (lambda parameters : parameters[label] - np.log(sum(np.exp(parameters))))\n",
    "\n",
    "def fprime_multinomial_log(label):\n",
    "    def _fprime_multinomial_log(parameters):\n",
    "        fprime = [-f_multinomial_pos(i)(parameters) for i in range(len(parameters))]\n",
    "        fprime[label] += 1\n",
    "        return fprime\n",
    "    return _fprime_multinomial_log\n",
    "    \n",
    "test_multinomial(f_multinomial_log, fprime_multinomial_log)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the gradient\n",
    "The log likelihood of data point $i$ can be written as $L(\\Theta) = \\log P(Y_i, Z_i;\\Theta)$. \n",
    "So the gradient of log-likelihood function can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\partial_{\\Theta} L(\\Theta) = \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(parameters, data):\n",
    "    \"\"\"Compute the likelihood and its gradients.\n",
    "    \n",
    "    arguments:\n",
    "    parameters -- current value of parameters\n",
    "    data     -- values for all random variables\n",
    "    \"\"\"\n",
    "    p_n, p_m = parameters\n",
    "    likelihood = 0\n",
    "    grad_n = [0.0] * len(p_n[0])\n",
    "    grad_m = [0.0] * len(p_m)\n",
    "    for instance in data:\n",
    "        label, value = instance\n",
    "        likelihood += f_normal_log(value)(p_n[label])\n",
    "        grad_n += fprime_normal_log(value)(p_n[label])\n",
    "        likelihood += f_multinomial_log(label)(p_m)\n",
    "        grad_m += fprime_multinomial_log(label)(p_m)\n",
    "    \n",
    "    gradients = grad_n, grad_m\n",
    "    return likelihood, gradients\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very important observation\n",
    "The differences in the values of `multinomial_parameters` leads to exponential differences, and does not let samples from other classes to be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data():\n",
    "    num_classes = 2\n",
    "    normal_parameters = [\n",
    "        [10, 5],\n",
    "        [25, 3]\n",
    "    ]\n",
    "    multinomial_parameters = [15.5, 16]\n",
    "    p2 = np.exp(multinomial_parameters)\n",
    "    p2 = [i/sum(p2) for i in p2]\n",
    "    \n",
    "    num_data = 1000\n",
    "    values = []\n",
    "    labels = []\n",
    "    for i in range(num_data):\n",
    "        label = np.random.multinomial(1, p2)\n",
    "        label = np.where(label==1)[0][0]\n",
    "        mu, sigma = normal_parameters[label]\n",
    "        value = np.random.normal(mu, sigma)\n",
    "        values.append(value)\n",
    "        labels.append(label)\n",
    "    return values, labels\n",
    "\n",
    "values, labels = generate_data()\n",
    "data = zip(labels, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff374138610>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD/CAYAAADoiI2GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3747dc750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "plt.hist(values,100)\n",
    "x = np.linspace(-20, 50, num=100)\n",
    "y1 = norm.pdf(x, loc=10, scale=5) * 200\n",
    "y3 = norm.pdf(x, loc=25, scale=3) * 300\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the class labels are hidden, we only have access to `values`. In each iteration, we complete the data using current parameters, compute the likelihood and gradient of completed data, and update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[10, 5], [25, 3]], [15.5, 16])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'f_normal_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-dcec4efd1e92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormal_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultinomial_parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-117-dcec4efd1e92>\u001b[0m in \u001b[0;36miterate\u001b[1;34m(parameters)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mcompleted_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mlikelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompleted_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-2e4246fee8af>\u001b[0m in \u001b[0;36mll\u001b[1;34m(parameters, data)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mlikelihood\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mf_normal_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mgrad_n\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfprime_normal_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlikelihood\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mf_multinomial_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'f_normal_log' is not defined"
     ]
    }
   ],
   "source": [
    "# returns a vector with counts of each class\n",
    "def complete_data(parameters, value, num_samples=1):\n",
    "    normal_parameters, multinomial_parameters = parameters\n",
    "    assert len(multinomial_parameters) == len(normal_parameters)\n",
    "    num_classes = len(multinomial_parameters)\n",
    "    p = np.exp(multinomial_parameters)\n",
    "    p = [pj/sum(p) for pj in p]\n",
    "    \n",
    "    posterior = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        mu, sigma = normal_parameters[i]\n",
    "        posterior[i] = norm.pdf(value, loc=mu, scale=sigma) * p[i]\n",
    "    s = sum(posterior)\n",
    "    posterior = [ps/s for ps in posterior]\n",
    "    labels =  np.random.multinomial(num_samples, posterior)\n",
    "    return [(label, value) for label in labels]\n",
    "\n",
    "def iterate(parameters):\n",
    "    completed_data = []\n",
    "    for value in values:\n",
    "        completed_data.append(complete_data(parameters, value))\n",
    "    likelihood, gradients = ll(parameters, completed_data)\n",
    "    parameters = parameters - alpha * gradients\n",
    "    return parameters\n",
    "\n",
    "normal_parameters = [\n",
    "    [10, 5],\n",
    "    [25, 3]\n",
    "]\n",
    "\n",
    "multinomial_parameters = [15.5, 16]\n",
    "parameters = normal_parameters, multinomial_parameters\n",
    "print parameters\n",
    "print iterate(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
