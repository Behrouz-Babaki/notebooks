{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- implement the gradient-based learning using sampling\n",
    "- implement the EM method\n",
    "- compare the two using synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Our goal in this exercise is to learn the parameters of a model from incomplete data. The model is a mixture of Guassians. According to this model, each data point is sampled from one of $C$ Gaussian distributions, according to the class of that data point. The prior probability for class label follows a multinomial distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Z_i = j) = p_j \n",
    "\\end{equation}\n",
    "\n",
    "Given the class of a data point, its value follows a normal distribution with parameters that are specific to that class:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y_i = y | Z_i = j) = N(y; \\mu_j, \\sigma_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the latent variables\n",
    "For each data point $Y_i$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Z_i=j|Y_i=y_i;\\Theta) & \\propto P(Y_i=y_i|Z_i=j;\\Theta)P(Z_i=j;\\Theta)   \\\\\n",
    "                  & = N(y_i;\\mu_j,\\sigma_j) \\cdot p_j\n",
    "\\end{align*}\n",
    "\n",
    "Having the posterior of class label, we can sample the label from this posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy.random import multinomial\n",
    "\n",
    "# returns a vector with counts of each class\n",
    "def sample_label(parameters, y, num_samples=1):\n",
    "    multinomial_parameters, normal_parameters = parameters\n",
    "    assert len(multinomial_parameters) == len(normal_parameters)\n",
    "    num_classes = len(multinomial_parameters)\n",
    "    \n",
    "    posterior = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        mu, sigma = normal_parameters[i]\n",
    "        p = multinomial_parameters[i]\n",
    "        posterior[i] = norm.pdf(y, loc=mu, scale=sigma) * p\n",
    "    s = sum(posterior)\n",
    "    posterior = [ps/s for ps in posterior]\n",
    "    return multinomial(num_samples, posterior)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us test the correctness of gradients of normal distribution. Given $f(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$, I believe that:\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\mu} &= f . \\frac{1}{\\sigma} . \\big(\\frac{x-\\mu}{\\sigma}\\big) \\\\\n",
    "\\frac{\\partial f}{\\partial \\sigma} &= f . \\frac{1}{\\sigma} . \\big( (\\frac{x-\\mu}{\\sigma})^2 -1 \\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "from scipy.stats import norm\n",
    "from numpy.random import random_sample, normal\n",
    "\n",
    "def f_normal(x):\n",
    "    def _f(inp):\n",
    "        mu, sigma = inp\n",
    "        return norm.pdf(x, loc=mu, scale=sigma)\n",
    "    return _f\n",
    "\n",
    "def fprime_normal(x):\n",
    "    def _fprime(inp):\n",
    "        f_val = f(x)(inp)\n",
    "        mu, sigma = inp\n",
    "        xms = (x-mu)/sigma\n",
    "        \n",
    "        grad=[0.0]*2\n",
    "        grad[0] = f_val/sigma * xms\n",
    "        grad[1] = f_val/sigma * (xms**2 - 1)\n",
    "        return grad\n",
    "    return _fprime\n",
    "\n",
    "# for i in range(10000):\n",
    "#     mu = 30 * random_sample()\n",
    "#     sigma = 10 * random_sample()\n",
    "#     val = normal(mu, sigma)\n",
    "#     diff =  check_grad(f(val), fprime(val), [mu, sigma])\n",
    "#     if diff > 1e-4:\n",
    "#         print mu, sigma, val, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25418880664 0.00285773272798 4.25605212501 0.000946297334813\n",
      "26.9849993887 0.00677352842907 26.9887627704 0.000162838865778\n",
      "1.69575405388 0.0244058934429 1.76135512501 0.000258878376052\n",
      "12.7578591462 0.0332553136623 12.8460135123 0.000135462727592\n",
      "28.4685679591 0.0202853643298 28.407044312 0.000481898145941\n",
      "21.1349210129 0.00742789160595 21.1305847481 0.000135073034998\n",
      "11.0491514474 0.00527095149876 11.0525998011 0.000278777309868\n",
      "6.28411926802 0.00311792143788 6.28574663193 0.00077910259721\n",
      "23.5841994442 0.000699097810046 23.582846381 0.156808649006\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random_sample, normal\n",
    "from scipy.stats import norm\n",
    "\n",
    "def f_normal_log(x):\n",
    "    return lambda y : norm.logpdf(x, loc=y[0], scale=y[1])\n",
    "\n",
    "def fprime_normal_log(x):\n",
    "    def fprime_log_(y):\n",
    "        mu, sigma = y\n",
    "        xms = (x-mu)/sigma\n",
    "        fprime_mu = xms / sigma\n",
    "        fprime_sigma = (xms**2 - 1) / sigma\n",
    "        return (fprime_mu, fprime_sigma)\n",
    "    return fprime_log_\n",
    "\n",
    "for i in range(5000):\n",
    "    mu = 30 * random_sample()\n",
    "    sigma = 10 * random_sample()\n",
    "    val = normal(mu, sigma)\n",
    "    diff =  check_grad(f_normal_log(val), fprime_normal_log(val), [mu, sigma])\n",
    "    if diff > 1e-4:\n",
    "        print mu, sigma, val, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_multinomial(f_multinomial_pos, fprime_multinomial_pos, \n",
    "                    num_tests = 1000):\n",
    "    num_classes = np.random.randint(40)\n",
    "    for i in range(num_tests):\n",
    "        params = [normal(loc=0, scale=1) for i in range(num_classes)]\n",
    "        p2 = np.exp(params)\n",
    "        p2 = [i/sum(p2) for i in p2]\n",
    "        val = np.random.multinomial(1, p2)\n",
    "        val = np.where(val==1)[0][0]\n",
    "        diff =  check_grad(f_multinomial_pos(val), fprime_multinomial_pos(val), params)\n",
    "        if diff > 1e-4:\n",
    "            print params, val, diff\n",
    "            print f_multinomial_pos(val)(params) \n",
    "            print fprime_multinomial_pos(val)(params)\n",
    "            print approx_fprime(params, f_multinomial_pos(val), 1.49e-08)\n",
    "            print \n",
    "    print 'test finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test finished\n"
     ]
    }
   ],
   "source": [
    "def f_multinomial_pos(label):\n",
    "    return (lambda parameters : np.exp(parameters[label]) / sum(np.exp(parameters)))\n",
    "\n",
    "def f_multinomial_log(label):\n",
    "    return (lambda parameters : parameters[label] - np.log(sum(np.exp(parameters))))\n",
    "\n",
    "def fprime_multinomial_log(label):\n",
    "    def _fprime_multinomial_log(parameters):\n",
    "        fprime = [-f_multinomial_pos(i)(parameters) for i in range(len(parameters))]\n",
    "        fprime[label] += 1\n",
    "        return fprime\n",
    "    return _fprime_multinomial_log\n",
    "    \n",
    "test_multinomial(f_multinomial_log, fprime_multinomial_log)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the gradient\n",
    "The log likelihood of data point $i$ can be written as $L(\\Theta) = \\log P(Y_i, Z_i;\\Theta)$. \n",
    "So the gradient of log-likelihood function can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\partial_{\\Theta} L(\\Theta) = \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very important observation\n",
    "The differences in the values of `multinomial_parameters` leads to exponential differences, and does not let samples from other classes to be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data():\n",
    "    num_classes = 2\n",
    "    normal_parameters = [\n",
    "        [10, 5],\n",
    "        [25, 3]\n",
    "    ]\n",
    "    multinomial_parameters = [15.5, 16]\n",
    "    p2 = np.exp(multinomial_parameters)\n",
    "    p2 = [i/sum(p2) for i in p2]\n",
    "    \n",
    "    num_data = 1000\n",
    "    values = []\n",
    "    labels = []\n",
    "    for i in range(num_data):\n",
    "        label = np.random.multinomial(1, p2)\n",
    "        label = np.where(label==1)[0][0]\n",
    "        mu, sigma = normal_parameters[label]\n",
    "        value = np.random.normal(mu, sigma)\n",
    "        values.append(value)\n",
    "        labels.append(label)\n",
    "    return values, labels\n",
    "\n",
    "values, labels = generate_data()\n",
    "data = zip(labels, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff374138610>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD/CAYAAADoiI2GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3747dc750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "plt.hist(values,100)\n",
    "x = np.linspace(-20, 50, num=100)\n",
    "y1 = norm.pdf(x, loc=10, scale=5) * 200\n",
    "y3 = norm.pdf(x, loc=25, scale=3) * 300\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the class labels are hidden, we only have access to `values`. In each iteration, we complete the data using current parameters, compute the likelihood and gradient of completed data, and update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(parameters, data):\n",
    "    \"\"\"Compute the likelihood and its gradients.\n",
    "    \n",
    "    arguments:\n",
    "    parameters -- current value of parameters\n",
    "    data     -- values for all random variables\n",
    "    \"\"\"\n",
    "    p_n, p_m = parameters\n",
    "    num_classes = len(p_m)\n",
    "    likelihood = 0\n",
    "    grad_n = [[0.0] * len(p_n[0])] * num_classes\n",
    "    grad_m = [0.0] * num_classes\n",
    "    for instance in data:\n",
    "        label, value = instance\n",
    "        likelihood += f_normal_log(value)(p_n[label])\n",
    "        fprime_n = fprime_normal_log(value)(p_n[label])\n",
    "        grad_n[label] = [grad_n[label][i] + fprime_n[i] for i in range(num_classes)]\n",
    "        \n",
    "        likelihood += f_multinomial_log(label)(p_m)\n",
    "        fprime_m = fprime_multinomial_log(label)(p_m)\n",
    "        grad_m = [grad_m[i]+fprime_m[i] for i in range(num_classes)]\n",
    "    \n",
    "    gradients = grad_n, grad_m\n",
    "    return likelihood, gradients\n",
    "\n",
    "\n",
    "# unnecessary complication\n",
    "def complete_data(values, parameters, num_samples=2):\n",
    "    completed_data = []\n",
    "    for value in values:\n",
    "        completed_data += get_samples(value, parameters, num_samples)\n",
    "    return completed_data\n",
    "\n",
    "def get_samples(value, parameters, num_samples):\n",
    "    p_n, p_m = parameters\n",
    "    num_classes = len(p_m)\n",
    "    prior = np.exp(p_m)\n",
    "    prior = [p/sum(prior) for p in prior]\n",
    "    \n",
    "    posterior = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        mu, sigma = p_n[i]\n",
    "        posterior[i] = norm.pdf(value, loc=mu, scale=sigma) * prior[i]\n",
    "    posterior = [p/sum(posterior) for p in posterior]\n",
    "\n",
    "    samples = np.random.multinomial(num_samples, posterior)\n",
    "    completed = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(samples[i]):\n",
    "            completed.append((i, value))\n",
    "    return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "normal_parameters = [\n",
    "    [15.5, 5],\n",
    "    [15, 5]\n",
    "]\n",
    "multinomial_parameters = [15.7, 15.8]\n",
    "parameters = [normal_parameters, multinomial_parameters]    \n",
    "completed_data = complete_data(values, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9521.09675332\n",
      "-9503.76033738\n",
      "-9486.94632741\n",
      "-9470.63053866\n",
      "-9454.79029713\n",
      "-9439.40432155\n",
      "-9424.45261635\n",
      "-9409.91637444\n",
      "-9395.77788873\n",
      "-9382.02047159\n",
      "-9368.62838126\n",
      "-9355.58675464\n",
      "-9342.88154579\n",
      "-9330.49946958\n",
      "-9318.42794992\n",
      "-9306.65507233\n",
      "-9295.16954016\n",
      "-9283.96063438\n",
      "-9273.01817646\n",
      "-9262.33249412\n",
      "-9251.89438968\n",
      "-9241.69511075\n",
      "-9231.72632316\n",
      "-9221.98008576\n",
      "-9212.44882717\n",
      "-9203.12532406\n",
      "-9194.00268105\n",
      "-9185.07431192\n",
      "-9176.33392222\n",
      "-9167.77549295\n",
      "-9159.39326539\n",
      "-9151.18172695\n",
      "-9143.1355979\n",
      "-9135.24981898\n",
      "-9127.51953981\n",
      "-9119.94010804\n",
      "-9112.50705915\n",
      "-9105.2161069\n",
      "-9098.06313436\n",
      "-9091.0441855\n",
      "-9084.15545725\n",
      "-9077.39329208\n",
      "-9070.75417092\n",
      "-9064.23470665\n",
      "-9057.8316378\n",
      "-9051.54182268\n",
      "-9045.36223389\n",
      "-9039.28995303\n",
      "-9033.3221658\n",
      "-9027.45615727\n",
      "-9021.68930754\n",
      "-9016.01908747\n",
      "-9010.44305476\n",
      "-9004.95885023\n",
      "-8999.56419418\n",
      "-8994.25688309\n",
      "-8989.03478637\n",
      "-8983.89584334\n",
      "-8978.83806036\n",
      "-8973.85950804\n",
      "-8968.95831868\n",
      "-8964.13268377\n",
      "-8959.38085162\n",
      "-8954.70112511\n",
      "-8950.09185959\n",
      "-8945.55146079\n",
      "-8941.07838291\n",
      "-8936.67112677\n",
      "-8932.328238\n",
      "-8928.04830541\n",
      "-8923.82995934\n",
      "-8919.67187014\n",
      "-8915.57274668\n",
      "-8911.53133499\n",
      "-8907.54641685\n",
      "-8903.61680858\n",
      "-8899.74135977\n",
      "-8895.91895208\n",
      "-8892.14849817\n",
      "-8888.4289406\n",
      "-8884.75925077\n",
      "-8881.13842798\n",
      "-8877.56549842\n",
      "-8874.03951431\n",
      "-8870.55955303\n",
      "-8867.12471622\n",
      "-8863.73412904\n",
      "-8860.38693938\n",
      "-8857.08231711\n",
      "-8853.81945336\n",
      "-8850.59755986\n",
      "-8847.41586826\n",
      "-8844.27362951\n",
      "-8841.17011324\n",
      "-8838.10460718\n",
      "-8835.07641662\n",
      "-8832.08486382\n",
      "-8829.12928751\n",
      "-8826.20904243\n",
      "-8823.32349876\n"
     ]
    }
   ],
   "source": [
    "def f_multinomial_pos(label):\n",
    "    return (lambda parameters : np.exp(parameters[label]) / sum(np.exp(parameters)))\n",
    "\n",
    "def f_multinomial_log(label):\n",
    "    return (lambda parameters : parameters[label] - np.log(sum(np.exp(parameters))))\n",
    "\n",
    "def fprime_multinomial_log(label):\n",
    "    def _fprime_multinomial_log(parameters):\n",
    "        fprime = [-f_multinomial_pos(i)(parameters) for i in range(len(parameters))]\n",
    "        fprime[label] += 1\n",
    "        return fprime\n",
    "    return _fprime_multinomial_log\n",
    "\n",
    "def ll_test(parameters, data):\n",
    "    \"\"\"Compute the likelihood and its gradients.\n",
    "    \n",
    "    arguments:\n",
    "    parameters -- current value of parameters\n",
    "    data     -- values for all random variables\n",
    "    \"\"\"\n",
    "    p_n, p_m = parameters\n",
    "    num_classes = len(p_m)\n",
    "    likelihood = 0\n",
    "    grad_n = [[0.0] * len(p_n[0])] * num_classes\n",
    "    grad_m = [0.0] * num_classes\n",
    "    for instance in data:\n",
    "        label, value = instance\n",
    "        likelihood += f_normal_log(value)(p_n[label])\n",
    "        fprime_n = fprime_normal_log(value)(p_n[label])\n",
    "        grad_n[label] = [grad_n[label][i] + fprime_n[i] for i in range(num_classes)]\n",
    "        \n",
    "        likelihood += f_multinomial_log(label)(p_m)\n",
    "        fprime_m = fprime_multinomial_log(label)(p_m)\n",
    "        grad_m = [grad_m[i]+fprime_m[i] for i in range(num_classes)]\n",
    "    \n",
    "    gradients = grad_n, grad_m\n",
    "    return likelihood, gradients\n",
    "\n",
    "current_params = parameters\n",
    "for iterations in range(100):\n",
    "    likelihood, gradients = ll_test(current_params, completed_data)\n",
    "    print (likelihood)\n",
    "    g_n, g_m = gradients\n",
    "    nu = 0.1 / len(completed_data)\n",
    "    p_n, p_m = parameters\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            p_n[i][j] += nu * g_n[i][j]\n",
    "    for i in range(2):\n",
    "        p_m[i] += nu * g_m[i]\n",
    "    current_params = (p_n, p_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
