{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- implement the gradient-based learning using sampling\n",
    "- implement the EM method\n",
    "- compare the two using synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Our goal in this exercise is to learn the parameters of a model from incomplete data. The model is a mixture of Guassians. According to this model, each data point is sampled from one of $C$ Gaussian distributions, according to the class of that data point. The prior probability for class label follows a multinomial distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Z_i = j) = p_j \n",
    "\\end{equation}\n",
    "\n",
    "Given the class of a data point, its value follows a normal distribution with parameters that are specific to that class:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Y_i = y | Z_i = j) = N(y; \\mu_j, \\sigma_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the latent variables\n",
    "For each data point $Y_i$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Z_i=j|Y_i=y_i;\\Theta) & \\propto P(Y_i=y_i|Z_i=j;\\Theta)P(Z_i=j;\\Theta)   \\\\\n",
    "                  & = N(y_i;\\mu_j,\\sigma_j) \\cdot p_j\n",
    "\\end{align*}\n",
    "\n",
    "Having the posterior of class label, we can sample the label from this posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from numpy.random import multinomial\n",
    "\n",
    "# returns a vector with counts of each class\n",
    "def sample_label(parameters, y, num_samples=1):\n",
    "    multinomial_parameters, normal_parameters = parameters\n",
    "    assert len(multinomial_parameters) == len(normal_parameters)\n",
    "    num_classes = len(multinomial_parameters)\n",
    "    \n",
    "    posterior = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        mu, sigma = normal_parameters[i]\n",
    "        p = multinomial_parameters[i]\n",
    "        posterior[i] = norm.pdf(y, loc=mu, scale=sigma) * p\n",
    "    s = sum(posterior)\n",
    "    posterior = [ps/s for ps in posterior]\n",
    "    return multinomial(num_samples, posterior)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let us test the correctness of gradients of normal distribution. Given $f(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{\\frac{(x-\\mu)^2}{2\\sigma^2}}$, I believe that:\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\mu} &= f . \\frac{1}{\\sigma} . \\big(\\frac{x-\\mu}{\\sigma}\\big) \\\\\n",
    "\\frac{\\partial f}{\\partial \\sigma} &= f . \\frac{1}{\\sigma} . \\big( (\\frac{x-\\mu}{\\sigma})^2 -1 \\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "from scipy.stats import norm\n",
    "from numpy.random import random_sample, normal\n",
    "\n",
    "def f_normal(x):\n",
    "    def _f(inp):\n",
    "        mu, sigma = inp\n",
    "        return norm.pdf(x, loc=mu, scale=sigma)\n",
    "    return _f\n",
    "\n",
    "def fprime_normal(x):\n",
    "    def _fprime(inp):\n",
    "        f_val = f(x)(inp)\n",
    "        mu, sigma = inp\n",
    "        xms = (x-mu)/sigma\n",
    "        \n",
    "        grad=[0.0]*2\n",
    "        grad[0] = f_val/sigma * xms\n",
    "        grad[1] = f_val/sigma * (xms**2 - 1)\n",
    "        return grad\n",
    "    return _fprime\n",
    "\n",
    "# for i in range(10000):\n",
    "#     mu = 30 * random_sample()\n",
    "#     sigma = 10 * random_sample()\n",
    "#     val = normal(mu, sigma)\n",
    "#     diff =  check_grad(f(val), fprime(val), [mu, sigma])\n",
    "#     if diff > 1e-4:\n",
    "#         print mu, sigma, val, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.588712872 0.00655657007391 12.5849768988 0.000173359388098\n",
      "20.8698218693 0.00750270114724 20.8736271925 0.000135742236687\n",
      "9.53332250902 0.00670702051321 9.53092919725 0.000194723599488\n",
      "2.38252437288 0.00995676100587 2.38294802532 0.00010597782716\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import random_sample, normal\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "def f_normal_log(x):\n",
    "    return lambda y : norm.logpdf(x, loc=y[0], scale=y[1])\n",
    "\n",
    "def fprime_normal_log(x):\n",
    "    def fprime_log_(y):\n",
    "        mu, sigma = y\n",
    "        xms = (x-mu)/sigma\n",
    "        fprime_mu = xms / sigma\n",
    "        fprime_sigma = (xms**2 - 1) / sigma\n",
    "        return (fprime_mu, fprime_sigma)\n",
    "    return fprime_log_\n",
    "\n",
    "for i in range(5000):\n",
    "    mu = 30 * random_sample()\n",
    "    sigma = 10 * random_sample()\n",
    "    val = normal(mu, sigma)\n",
    "    diff =  check_grad(f_normal_log(val), fprime_normal_log(val), [mu, sigma])\n",
    "    if diff > 1e-4:\n",
    "        print mu, sigma, val, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_multinomial(f_multinomial_pos, fprime_multinomial_pos, \n",
    "                    num_tests = 1000):\n",
    "    num_classes = np.random.randint(40)\n",
    "    for i in range(num_tests):\n",
    "        params = [normal(loc=0, scale=1) for i in range(num_classes)]\n",
    "        p2 = np.exp(params)\n",
    "        p2 = [i/sum(p2) for i in p2]\n",
    "        val = np.random.multinomial(1, p2)\n",
    "        val = np.where(val==1)[0][0]\n",
    "        diff =  check_grad(f_multinomial_pos(val), fprime_multinomial_pos(val), params)\n",
    "        if diff > 1e-4:\n",
    "            print params, val, diff\n",
    "            print f_multinomial_pos(val)(params) \n",
    "            print fprime_multinomial_pos(val)(params)\n",
    "            print approx_fprime(params, f_multinomial_pos(val), 1.49e-08)\n",
    "            print \n",
    "    print 'test finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test finished\n"
     ]
    }
   ],
   "source": [
    "def f_multinomial_pos(label):\n",
    "    return (lambda parameters : np.exp(parameters[label]) / sum(np.exp(parameters)))\n",
    "\n",
    "def f_multinomial_log(label):\n",
    "    return (lambda parameters : parameters[label] - np.log(sum(np.exp(parameters))))\n",
    "\n",
    "def fprime_multinomial_log(label):\n",
    "    def _fprime_multinomial_log(parameters):\n",
    "        fprime = [-f_multinomial_pos(i)(parameters) for i in range(len(parameters))]\n",
    "        fprime[label] += 1\n",
    "        return fprime\n",
    "    return _fprime_multinomial_log\n",
    "    \n",
    "test_multinomial(f_multinomial_log, fprime_multinomial_log)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating the gradient\n",
    "The log likelihood of data point $i$ can be written as $L(\\Theta) = \\log P(Y_i, Z_i;\\Theta)$. \n",
    "So the gradient of log-likelihood function can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\partial_{\\Theta} L(\\Theta) = \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very important observation\n",
    "The differences in the values of `multinomial_parameters` leads to exponential differences, and does not let samples from other classes to be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data():\n",
    "    num_classes = 2\n",
    "    normal_parameters = [\n",
    "        [10, 5],\n",
    "        [25, 3]\n",
    "    ]\n",
    "    multinomial_parameters = [15.5, 16]\n",
    "    p2 = np.exp(multinomial_parameters)\n",
    "    p2 = [i/sum(p2) for i in p2]\n",
    "    \n",
    "    num_data = 1000\n",
    "    values = []\n",
    "    labels = []\n",
    "    for i in range(num_data):\n",
    "        label = np.random.multinomial(1, p2)\n",
    "        label = np.where(label==1)[0][0]\n",
    "        mu, sigma = normal_parameters[label]\n",
    "        value = np.random.normal(mu, sigma)\n",
    "        values.append(value)\n",
    "        labels.append(label)\n",
    "    return values, labels\n",
    "\n",
    "values, labels = generate_data()\n",
    "data = zip(labels, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f90e4819b90>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD/CAYAAADoiI2GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDyIRtLcAx4JyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90e48974d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "plt.hist(values,100)\n",
    "x = np.linspace(-20, 50, num=100)\n",
    "y1 = norm.pdf(x, loc=10, scale=5) * 200\n",
    "y3 = norm.pdf(x, loc=25, scale=3) * 300\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the class labels are hidden, we only have access to `values`. In each iteration, we complete the data using current parameters, compute the likelihood and gradient of completed data, and update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(parameters, data):\n",
    "    \"\"\"Compute the likelihood and its gradients.\n",
    "    \n",
    "    arguments:\n",
    "    parameters -- current value of parameters\n",
    "    data     -- values for all random variables\n",
    "    \"\"\"\n",
    "    p_n, p_m = parameters\n",
    "    num_classes = len(p_m)\n",
    "    likelihood = 0\n",
    "    grad_n = [[0.0] * len(p_n[0])] * num_classes\n",
    "    grad_m = [0.0] * num_classes\n",
    "    for instance in data:\n",
    "        label, value = instance\n",
    "        likelihood += f_normal_log(value)(p_n[label])\n",
    "        fprime_n = fprime_normal_log(value)(p_n[label])\n",
    "        grad_n[label] = [grad_n[label][i] + fprime_n[i] for i in range(num_classes)]\n",
    "        \n",
    "        likelihood += f_multinomial_log(label)(p_m)\n",
    "        fprime_m = fprime_multinomial_log(label)(p_m)\n",
    "        grad_m = [grad_m[i]+fprime_m[i] for i in range(num_classes)]\n",
    "    \n",
    "    gradients = grad_n, grad_m\n",
    "    return likelihood, gradients\n",
    "\n",
    "\n",
    "# unnecessary complication\n",
    "def complete_data(values, parameters, num_samples=2):\n",
    "    completed_data = []\n",
    "    for value in values:\n",
    "        completed_data += get_samples(value, parameters, num_samples)\n",
    "    return completed_data\n",
    "\n",
    "def get_samples(value, parameters, num_samples):\n",
    "    p_n, p_m = parameters\n",
    "    num_classes = len(p_m)\n",
    "    prior = np.exp(p_m)\n",
    "    prior = [p/sum(prior) for p in prior]\n",
    "    \n",
    "    posterior = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        mu, sigma = p_n[i]\n",
    "        posterior[i] = norm.pdf(value, loc=mu, scale=sigma) * prior[i]\n",
    "    posterior = [p/sum(posterior) for p in posterior]\n",
    "\n",
    "    samples = np.random.multinomial(num_samples, posterior)\n",
    "    completed = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(samples[i]):\n",
    "            completed.append((i, value))\n",
    "    return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "normal_parameters = [\n",
    "    [15.5, 5],\n",
    "    [15, 5]\n",
    "]\n",
    "multinomial_parameters = [15.7, 15.8]\n",
    "parameters = [normal_parameters, multinomial_parameters]    \n",
    "completed_data = complete_data(values, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def f_multinomial_pos(label):\n",
    "    return (lambda parameters : np.exp(parameters[label]) / sum(np.exp(parameters)))\n",
    "\n",
    "def f_multinomial_log(label):\n",
    "    return (lambda parameters : parameters[label] - np.log(sum(np.exp(parameters))))\n",
    "\n",
    "def fprime_multinomial_log(label):\n",
    "    def _fprime_multinomial_log(parameters):\n",
    "        fprime = [-f_multinomial_pos(i)(parameters) for i in range(len(parameters))]\n",
    "        fprime[label] += 1\n",
    "        return fprime\n",
    "    return _fprime_multinomial_log\n",
    "\n",
    "def ll_test(parameters, data):\n",
    "    \"\"\"Compute the likelihood and its gradients.\n",
    "    \n",
    "    arguments:\n",
    "    parameters -- current value of parameters\n",
    "    data     -- values for all random variables\n",
    "    \"\"\"\n",
    "    p_n, p_m = parameters\n",
    "    num_classes = len(p_m)\n",
    "    likelihood = 0\n",
    "    grad_n = [[0.0] * len(p_n[0])] * num_classes\n",
    "    grad_m = [0.0] * num_classes\n",
    "    for instance in data:\n",
    "        label, value = instance\n",
    "        likelihood += f_normal_log(value)(p_n[label])\n",
    "        fprime_n = fprime_normal_log(value)(p_n[label])\n",
    "        grad_n[label] = [grad_n[label][i] + fprime_n[i] for i in range(num_classes)]\n",
    "        \n",
    "        likelihood += f_multinomial_log(label)(p_m)\n",
    "        fprime_m = fprime_multinomial_log(label)(p_m)\n",
    "        grad_m = [grad_m[i]+fprime_m[i] for i in range(num_classes)]\n",
    "    \n",
    "    gradients = grad_n, grad_m\n",
    "    return likelihood, gradients\n",
    "\n",
    "current_params = parameters\n",
    "current_data = completed_data\n",
    "for iterations in range(100):\n",
    "    likelihood, gradients = ll_test(current_params, completed_data)\n",
    "    print (likelihood)\n",
    "    g_n, g_m = gradients\n",
    "    nu = 0.1 / len(completed_data)\n",
    "    p_n, p_m = parameters\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            p_n[i][j] += nu * g_n[i][j]\n",
    "    for i in range(2):\n",
    "        p_m[i] += nu * g_m[i]\n",
    "    current_params = (p_n, p_m)\n",
    "    current_data = complete_data(values, current_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
